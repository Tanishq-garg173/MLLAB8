# -*- coding: utf-8 -*-
"""MACHINE LEARNING ASSIGNMENT8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1V9HcSpLsjLumoctAZs1aHEj6suTXTLB5
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, log_loss, classification_report)
from imblearn.over_sampling import SMOTE
import xgboost
from sklearn.preprocessing import LabelEncoder

from google.colab import files
uploaded = files.upload()
data = pd.read_csv('WA_Fn-UseC_-HR-Employee-Attrition.csv')

data.head()

display(data.isnull().any()) #to check for any null values

"""No null values"""

data.hist(edgecolor='black', linewidth=1.5, figsize=(15,15));

data.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis="columns", inplace=True)
#EmployeeNumber is just for identification,so we can remove it.Rest columns which are removed do not change much as the value range changes.Hencd they can also be changed.Basically they hav a standard deviation of nearby zero.

from collections import Counter
print(Counter(data['Attrition']))
sns.countplot(x='Attrition', data=data)
plt.title("Division of Attrition")

"""Attrition is basically our label"""

categorical_col = []
for column in data.columns:
    if data[column].dtype == object and len(data[column].unique()) <= 50:
        categorical_col.append(column)
        print(f"{column} : {data[column].unique()}")
        print("====================================")

data['Attrition'] = data.Attrition.astype("category").cat.codes

graph = plt.figure(figsize = (8,4))
sns.barplot(x = 'Attrition', y = 'Attrition', data =data )

sns.set(font_scale=1.2)
plt.figure(figsize=(30, 30))

for i, column in enumerate(categorical_col, 1):
    plt.subplot(3, 3, i)
    g = sns.barplot(x=f"{column}", y='Attrition', data=data)
    g.set_xticklabels(g.get_xticklabels(), rotation=90)
    plt.ylabel('Attrition Count')
    plt.xlabel(f'{column}')

"""It is conclusive that the person who travels more is more likely ti quit the job.
Person who has a job in Research and development is less likely to quit than others.
Employee having gender as male are more likely to quit the job.
The workers in Laboratory Technician, Sales Representative, and Human Resources are more likely to quit the workers in other positions.
Single people are more likely to quit than married or divorced.
Employee with more working hours is more likely to quit.
"""

corr_matrix = data.corr()
print(corr_matrix["Attrition"].sort_values(ascending=False))

plt.figure(figsize=(30, 30))
sns.heatmap(data.corr(), annot=True, annot_kws={"size":15})

categorical_col.remove('Attrition')

"""IT IS DONE AS ATTRITION IS OUR LABEL"""

label = LabelEncoder()
for column in categorical_col:
    data[column] = label.fit_transform(data[column])

features= data.drop('Attrition', axis=1)
label= data.Attrition

pd.set_option('display.max_columns', None)
from sklearn.model_selection import train_test_split
features_train, features_test, label_train, label_test = train_test_split(features,label,test_size=0.20)
trcheck=features_train.head()
print(trcheck)
print(features_train.shape)

oversampler=SMOTE(random_state=0)
smote_train, smote_target = oversampler.fit_sample(features_train,label_train)

import warnings
warnings.filterwarnings('ignore')
seed = 0   
# Random Forest parameters
rf_params = {
    'n_jobs': -1,
    'n_estimators': 1000,
#     'warm_start': True, 
    'max_features': 0.3,
    'max_depth': 4,
    'min_samples_leaf': 2,
    'max_features' : 'sqrt',
    'random_state' : seed,
    'verbose': 0
}

rf = RandomForestClassifier(**rf_params)
rf.fit(smote_train, smote_target)

rf_predictions = rf.predict(features_test)

print("Accuracy score: {}".format(accuracy_score(label_test, rf_predictions)))
print("="*80)
print(classification_report(label_test, rf_predictions))

"""IMPORTANCE OF FEATURES"""

feat_importances = pd.Series(rf.feature_importances_, index=features.columns)
feat_importances = feat_importances.nlargest(20)
feat_importances.plot(kind='barh')

from sklearn import tree
dt = tree.DecisionTreeClassifier()
dt.fit(smote_train, smote_target)
print("Fitting of decision treefinished")

dt_predictions = dt.predict(features_test)
print("Predictions finished")

print("Accuracy score: {}".format(accuracy_score(label_test, dt_predictions)))
print("CLASSIFICATION REPORT")
print(classification_report(label_test, dt_predictions))

print("Feature importance")
feat_importances = pd.Series(dt.feature_importances_, index=features.columns)
feat_importances = feat_importances.nlargest(20)
feat_importances.plot(kind='barh')

gb_params ={
    'n_estimators': 1500,
    'max_features': 0.9,
    'learning_rate' : 0.25,
    'max_depth': 4,
    'min_samples_leaf': 2,
    'subsample': 1,
    'max_features' : 'sqrt',
    'random_state' : seed,
    'verbose': 0
}

gb = GradientBoostingClassifier(**gb_params)
# Fit the model to our SMOTEd train and target
gb.fit(smote_train, smote_target)
# Get our predictions
gb_predictions = gb.predict(features_test)

print(accuracy_score(label_test, gb_predictions))
print("="*80)
print(classification_report(label_test, gb_predictions))

"""FEATURE IMPORTANCE"""

feat_importances = pd.Series(gb.feature_importances_, index=features.columns)
feat_importances = feat_importances.nlargest(20)
feat_importances.plot(kind='barh')

"""AcCURACIES OBTAINED:
RANDOM FOREST:84.6
DECISION TREE:75.5
GRADIENT BOOSTING:89.4

Basically the difference between rando forest and decision tree is that in,decision tree the prediction is based on just 1 tree with all features.However,in random forest,multiple trees are generated by nottaking all the features in every decision tree.Therefore,we obtain high accuracy with random forest than decision tree.Random forest has low bias and variance.
"""